{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e9c6ac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:52:34.487726Z",
     "iopub.status.busy": "2023-10-01T17:52:34.487197Z",
     "iopub.status.idle": "2023-10-01T17:52:34.831153Z",
     "shell.execute_reply": "2023-10-01T17:52:34.829896Z"
    },
    "papermill": {
     "duration": 0.350276,
     "end_time": "2023-10-01T17:52:34.833142",
     "exception": false,
     "start_time": "2023-10-01T17:52:34.482866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UYARI ENGELLEYİCİ\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "print(\"UYARI ENGELLEYİCİ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f083c484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:52:34.841730Z",
     "iopub.status.busy": "2023-10-01T17:52:34.840678Z",
     "iopub.status.idle": "2023-10-01T17:52:34.991573Z",
     "shell.execute_reply": "2023-10-01T17:52:34.990349Z"
    },
    "papermill": {
     "duration": 0.157126,
     "end_time": "2023-10-01T17:52:34.993519",
     "exception": false,
     "start_time": "2023-10-01T17:52:34.836393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOSYA YOL KODLARI\n"
     ]
    }
   ],
   "source": [
    "filepath = '/kaggle/input/commonlit-evaluate-student-summaries'\n",
    "\n",
    "prompts_test = pd.read_csv(os.path.join(filepath, 'prompts_test.csv'))\n",
    "prompts_train = pd.read_csv(os.path.join(filepath, 'prompts_train.csv'))\n",
    "sub = pd.read_csv(os.path.join(filepath, 'sample_submission.csv'))\n",
    "summaries_test = pd.read_csv(os.path.join(filepath, 'summaries_test.csv'))\n",
    "summaries_train = pd.read_csv(os.path.join(filepath, 'summaries_train.csv'))\n",
    "print(\"DOSYA YOL KODLARI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b253418",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:52:35.001999Z",
     "iopub.status.busy": "2023-10-01T17:52:35.001273Z",
     "iopub.status.idle": "2023-10-01T17:53:04.800950Z",
     "shell.execute_reply": "2023-10-01T17:53:04.799698Z"
    },
    "papermill": {
     "duration": 29.806409,
     "end_time": "2023-10-01T17:53:04.803388",
     "exception": false,
     "start_time": "2023-10-01T17:52:34.996979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# NLTK'den tokenizer'ı yükle\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizasyon işlemi\n",
    "summaries_train['tokenized_text'] = summaries_train['text'].apply(lambda x: word_tokenize(x))\n",
    "prompts_test['prompt_text'] = prompts_test['prompt_text'].apply(lambda x: word_tokenize(x))\n",
    "summaries_test['text'] = summaries_test['text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab5c296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:53:04.811426Z",
     "iopub.status.busy": "2023-10-01T17:53:04.811115Z",
     "iopub.status.idle": "2023-10-01T17:53:30.057720Z",
     "shell.execute_reply": "2023-10-01T17:53:30.056881Z"
    },
    "papermill": {
     "duration": 25.25327,
     "end_time": "2023-10-01T17:53:30.060146",
     "exception": false,
     "start_time": "2023-10-01T17:53:04.806876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n",
      "0       third wave experimentto see people reacted new...\n",
      "1       would rub soda make smell go away wouldnt bad ...\n",
      "2       egypt  many occupations social classes involve...\n",
      "3       highest class pharaohs people gods then 2nd hi...\n",
      "4       third wave developed rapidly students genuinly...\n",
      "                              ...                        \n",
      "7160    used sorts chemical concoctions make meat seem...\n",
      "7161    lowest classes slaves farmers slaves people ta...\n",
      "7162    sorta made people start working structour they...\n",
      "7163    ideal tragety three elements make ideal  start...\n",
      "7164    meat would smell sour would  rub soda take awa...\n",
      "Name: cleaned_text, Length: 7165, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# NLTK stopwords verilerini indirin\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Temizleme işlemlerini uygulama fonksiyonu\n",
    "def preprocess_text(tokenized_text):\n",
    "    # Her bir kelimeyi temizle\n",
    "    cleaned_words = []\n",
    "    for word in tokenized_text:\n",
    "        # Küçük harfe dönüştürme\n",
    "        word = word.lower()\n",
    "        # Noktalama işaretlerini boşlukla değiştirme\n",
    "        word = re.sub(r'[^\\w\\s]', ' ', word)\n",
    "        # Gereksiz boşlukları kaldırma\n",
    "        word = re.sub(r'\\s+', ' ', word)\n",
    "        # Stopwords'leri kaldırma\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Tokenleştirilmiş metinleri temizleme\n",
    "summaries_train['cleaned_text'] = summaries_train['tokenized_text'].apply(preprocess_text)\n",
    "prompts_test['prompt_text'] = prompts_test['prompt_text'].apply(preprocess_text)\n",
    "summaries_test['text'] = summaries_test['text'].apply(preprocess_text)\n",
    "\n",
    "# Noktalama işaretlerini boşlukla değiştirme\n",
    "summaries_train['cleaned_text'] = summaries_train['cleaned_text'].str.replace('[^\\w\\s]', '')\n",
    "prompts_test['prompt_text'] = prompts_test['prompt_text'].str.replace('[^\\w\\s]', '')\n",
    "summaries_test['text'] = summaries_test['text'].str.replace('[^\\w\\s]', '')\n",
    "\n",
    "# ŞİMDİ BOŞLUKLARI KALDIRRRR HAYDİİİİİİİİİİİ\n",
    "summaries_train['cleaned_text'] = summaries_train['cleaned_text'].str.replace('\\s+', '')\n",
    "prompts_test['prompt_text'] = prompts_test['prompt_text'].str.replace('\\s+', '')\n",
    "summaries_test['text'] = summaries_test['text'].str.replace('\\s+', '')\n",
    "\n",
    "#Bişey deniyorum\n",
    "summaries_train['cleaned_text'] = summaries_train['cleaned_text'].str.replace('  ', ' ')\n",
    "prompts_test['prompt_text'] = prompts_test['prompt_text'].str.replace('  ', ' ')\n",
    "summaries_test['text'] = summaries_test['text'].str.replace('  ', ' ')\n",
    "\n",
    "summaries_train['cleaned_text'] = summaries_train['cleaned_text'].str.replace('   ', ' ')\n",
    "prompts_test['prompt_text'] = prompts_test['prompt_text'].str.replace('   ', ' ')\n",
    "summaries_test['text'] = summaries_test['text'].str.replace('   ', ' ')\n",
    "\n",
    "summaries_train['cleaned_text'] = summaries_train['cleaned_text'].str.replace('    ', ' ')\n",
    "prompts_test['prompt_text'] = prompts_test['prompt_text'].str.replace('    ', ' ')\n",
    "summaries_test['text'] = summaries_test['text'].str.replace('    ', ' ')\n",
    "\n",
    "# Temizlenmiş verileri gösterme\n",
    "print(summaries_train['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff6e5e9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:53:30.068521Z",
     "iopub.status.busy": "2023-10-01T17:53:30.068220Z",
     "iopub.status.idle": "2023-10-01T17:53:30.808934Z",
     "shell.execute_reply": "2023-10-01T17:53:30.807531Z"
    },
    "papermill": {
     "duration": 0.747469,
     "end_time": "2023-10-01T17:53:30.811217",
     "exception": false,
     "start_time": "2023-10-01T17:53:30.063748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    example      text\n",
      "0  0.707107  0.707107\n",
      "1  0.707107  0.707107\n",
      "2  0.707107  0.707107\n",
      "3  0.707107  0.707107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF vektörleştirmeyi yapılandırın\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Örnek olarak, en yaygın 1000 terimi kullanacağız\n",
    "tfidf_vectorizer_test = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# TF-IDF matrisini oluşturun\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(summaries_train['cleaned_text'])\n",
    "tfidf_matrix_test = tfidf_vectorizer_test.fit_transform(summaries_test['text'])\n",
    "\n",
    "# TF-IDF matrisini bir DataFrame olarak saklayabilirsiniz (isteğe bağlı)\n",
    "tf_idf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tf_idf_test = pd.DataFrame(tfidf_matrix_test.toarray(), columns=tfidf_vectorizer_test.get_feature_names_out())\n",
    "\n",
    "# TF-IDF matrisini inceleyin\n",
    "print(tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2688e68a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:53:30.820050Z",
     "iopub.status.busy": "2023-10-01T17:53:30.819385Z",
     "iopub.status.idle": "2023-10-01T17:53:30.937525Z",
     "shell.execute_reply": "2023-10-01T17:53:30.936145Z"
    },
    "papermill": {
     "duration": 0.124651,
     "end_time": "2023-10-01T17:53:30.939790",
     "exception": false,
     "start_time": "2023-10-01T17:53:30.815139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would      436.117878\n",
      "meat       407.681365\n",
      "tragedy    277.687854\n",
      "good       218.219429\n",
      "spoiled    210.656793\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Terimlerin sıklıklarını hesaplayın ve sıralayın\n",
    "term_frequencies = tf_idf.sum().sort_values(ascending=False)\n",
    "\n",
    "# En yaygın terimleri gösterin (örneğin, ilk 10 terimi alalım)\n",
    "top_terms = term_frequencies.head(5)\n",
    "\n",
    "# Sonuçları gösterin\n",
    "print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f15d998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:53:30.948515Z",
     "iopub.status.busy": "2023-10-01T17:53:30.948190Z",
     "iopub.status.idle": "2023-10-01T17:53:30.971795Z",
     "shell.execute_reply": "2023-10-01T17:53:30.970879Z"
    },
    "papermill": {
     "duration": 0.030263,
     "end_time": "2023-10-01T17:53:30.973772",
     "exception": false,
     "start_time": "2023-10-01T17:53:30.943509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>...</th>\n",
       "      <th>yearly</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>youd</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>zero</th>\n",
       "      <th>ziggurat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  100   11   12   13   14   15   16   17   18  ...  yearly  years  yes  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0    0.0  0.0   \n",
       "\n",
       "   yet  you  youd  young  youre  zero  ziggurat  \n",
       "0  0.0  0.0   0.0    0.0    0.0   0.0       0.0  \n",
       "\n",
       "[1 rows x 5000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488220f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:53:30.982627Z",
     "iopub.status.busy": "2023-10-01T17:53:30.982336Z",
     "iopub.status.idle": "2023-10-01T17:53:30.988308Z",
     "shell.execute_reply": "2023-10-01T17:53:30.987342Z"
    },
    "papermill": {
     "duration": 0.0126,
     "end_time": "2023-10-01T17:53:30.990177",
     "exception": false,
     "start_time": "2023-10-01T17:53:30.977577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7165, 5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e9ec82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:53:30.999359Z",
     "iopub.status.busy": "2023-10-01T17:53:30.999095Z",
     "iopub.status.idle": "2023-10-01T17:54:11.044313Z",
     "shell.execute_reply": "2023-10-01T17:54:11.043039Z"
    },
    "papermill": {
     "duration": 40.064791,
     "end_time": "2023-10-01T17:54:11.058893",
     "exception": false,
     "start_time": "2023-10-01T17:53:30.994102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3.4131609761588807e+21\n",
      "R-squared: -3.30938923322951e+21\n",
      "Content RMSE: 1.1926486609375508\n",
      "Wording RMSE: 1.1985810331041258\n",
      "MCRMSE: 1.1956148470208383\n",
      "******************************\n",
      "Ortalama RMSE: 1.2027654277826363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# X ve Y'yi ayırın\n",
    "X = tf_idf\n",
    "y = summaries_train[['content', 'wording']]\n",
    "\n",
    "# Veriyi eğitim ve test kümelerine ayırın\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Lineer regresyon modelini oluşturun\n",
    "model = LinearRegression()\n",
    "\n",
    "# Modeli eğitin\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin test verileri üzerinde performansını değerlendirin\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n",
    "\n",
    "def compt_score(content_true, content_pred, wording_true, wording_pred):\n",
    "    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n",
    "    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n",
    "    \n",
    "    return (content_score + wording_score)/2\n",
    "\n",
    "# Örnek tahminler\n",
    "random_preds = np.random.rand(len(y_test), 2)  # 2 sütunlu tahminler\n",
    "\n",
    "# MCRMSE hesaplama fonksiyonunu kullanarak sonuçları alın\n",
    "mcrmse_result = compute_mcrmse((random_preds, y_test.values))\n",
    "\n",
    "# MCRMSE sonuçlarını yazdırın\n",
    "print(\"Content RMSE:\", mcrmse_result[\"content_rmse\"])\n",
    "print(\"Wording RMSE:\", mcrmse_result[\"wording_rmse\"])\n",
    "print(\"MCRMSE:\", mcrmse_result[\"mcrmse\"])\n",
    "\n",
    "print(\"*\"*30)\n",
    "\n",
    "# Gerçek content ve wording değerleri\n",
    "content_true = y_test['content'].values\n",
    "wording_true = y_test['wording'].values\n",
    "\n",
    "# Tahmin edilen content ve wording değerleri (örnek olarak rastgele tahminler kullanıldı)\n",
    "content_pred = np.random.rand(len(y_test))\n",
    "wording_pred = np.random.rand(len(y_test))\n",
    "\n",
    "# `compt_score` fonksiyonunu kullanarak ortalama RMSE skorunu hesaplayın\n",
    "average_rmse = compt_score(content_true, content_pred, wording_true, wording_pred)\n",
    "\n",
    "# Sonucu yazdırın\n",
    "print(\"Ortalama RMSE:\", average_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b529965",
   "metadata": {
    "papermill": {
     "duration": 0.010194,
     "end_time": "2023-10-01T17:54:11.079664",
     "exception": false,
     "start_time": "2023-10-01T17:54:11.069470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ufak deneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c76831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:54:11.104435Z",
     "iopub.status.busy": "2023-10-01T17:54:11.102770Z",
     "iopub.status.idle": "2023-10-01T17:54:11.119306Z",
     "shell.execute_reply": "2023-10-01T17:54:11.117809Z"
    },
    "papermill": {
     "duration": 0.03306,
     "end_time": "2023-10-01T17:54:11.123222",
     "exception": false,
     "start_time": "2023-10-01T17:54:11.090162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Content ve wording tahminlerinin ortalamasını alın\n",
    "average_content = np.mean(content_pred)\n",
    "average_wording = np.mean(wording_pred)\n",
    "\n",
    "# Mevcut \"sub\" dosyasına tahminleri ekleyin\n",
    "sub['content'] = average_content\n",
    "sub['wording'] = average_wording\n",
    "\n",
    "# Güncellenmiş gönderim dosyasını kaydedin\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b011dc44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-01T17:54:11.147016Z",
     "iopub.status.busy": "2023-10-01T17:54:11.146436Z",
     "iopub.status.idle": "2023-10-01T17:54:11.157846Z",
     "shell.execute_reply": "2023-10-01T17:54:11.156859Z"
    },
    "papermill": {
     "duration": 0.025359,
     "end_time": "2023-10-01T17:54:11.159661",
     "exception": false,
     "start_time": "2023-10-01T17:54:11.134302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>0.503214</td>\n",
       "      <td>0.481397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>0.503214</td>\n",
       "      <td>0.481397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>0.503214</td>\n",
       "      <td>0.481397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>333333dddddd</td>\n",
       "      <td>0.503214</td>\n",
       "      <td>0.481397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id   content   wording\n",
       "0  000000ffffff  0.503214  0.481397\n",
       "1  111111eeeeee  0.503214  0.481397\n",
       "2  222222cccccc  0.503214  0.481397\n",
       "3  333333dddddd  0.503214  0.481397"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 100.273906,
   "end_time": "2023-10-01T17:54:11.782366",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-01T17:52:31.508460",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
